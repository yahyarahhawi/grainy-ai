{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from models import create_model\n",
    "from options.test_options import TestOptions\n",
    "\n",
    "# Path to your pre-trained weights\n",
    "weights_path = \"/Users/yahyarahhawi/Developer/Film/15_net_G_A.pth\"\n",
    "\n",
    "# Override sys.argv to inject minimal CLI args for TestOptions\n",
    "original_argv = sys.argv\n",
    "sys.argv = [\n",
    "    original_argv[0],\n",
    "    '--dataroot', 'dummy',        # Needed to satisfy required '--dataroot' argument\n",
    "    '--model', 'cycle_gan',       # Specifies the CycleGAN model\n",
    "    '--dataset_mode', 'single',   # We are feeding images manually\n",
    "    '--gpu_ids', '-1',            # Force CPU usage (no CUDA)\n",
    "]\n",
    "\n",
    "# Parse the options\n",
    "opt = TestOptions().parse()\n",
    "\n",
    "# Restore sys.argv to its original state\n",
    "sys.argv = original_argv\n",
    "\n",
    "# Set additional options manually\n",
    "opt.isTrain = False  # Indicates we're running in evaluation/test mode\n",
    "opt.no_dropout = True  # No dropout for inference\n",
    "opt.batch_size = 1  # Inference only works with batch size = 1\n",
    "opt.load_size = 256  # Resize to 256x256 during preprocessing (adjust as needed)\n",
    "opt.crop_size = 256  # Crop size must match load size for this example\n",
    "\n",
    "# Create the CycleGAN model\n",
    "cycleGAN = create_model(opt)\n",
    "cycleGAN.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Manually load netG_A weights\n",
    "state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
    "cycleGAN.netG_A.load_state_dict(state_dict)\n",
    "\n",
    "# Access the generator (netG_A)\n",
    "model_netG_A = cycleGAN.netG_A\n",
    "\n",
    "# Print a summary to verify everything is working\n",
    "print(\"CycleGAN model created successfully!\")\n",
    "print(f\"Generator loaded: {model_netG_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def export_cycleGAN_to_onnx(\n",
    "    model_netG_A: torch.nn.Module,\n",
    "    onnx_path: str = \"cycleGAN_genA.onnx\",\n",
    "    input_size: int = 256\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports the given netG_A generator to ONNX format.\n",
    "\n",
    "    :param model_netG_A: The PyTorch nn.Module for netG_A (e.g., cycleGAN.netG_A).\n",
    "    :param onnx_path:    Where to save the ONNX file.\n",
    "    :param input_size:   The (height, width) for the input image.\n",
    "    \"\"\"\n",
    "    # Put the model in eval mode\n",
    "    model_netG_A.eval()\n",
    "\n",
    "    # Create a dummy input of shape [batch_size=1, channels=3, height=input_size, width=input_size]\n",
    "    dummy_input = torch.randn(1, 3, input_size, input_size, device=\"cpu\")\n",
    "\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model_netG_A,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        opset_version=11,  # Adjust opset version if needed (11 is widely compatible)\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "\n",
    "    print(f\"ONNX model saved to {onnx_path}\")\n",
    "\n",
    "\n",
    "# Export the loaded generator (netG_A) to ONNX\n",
    "onnx_output_path = \"/Users/yahyarahhawi/Developer/Film/cycleGAN_genA.onnx\"\n",
    "export_cycleGAN_to_onnx(model_netG_A=cycleGAN.netG_A, onnx_path=onnx_output_path, input_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a dummy input with the same shape as your input images\n",
    "dummy_input = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Trace the model to produce a TorchScript version\n",
    "traced_model = torch.jit.trace(model_netG_A, dummy_input)\n",
    "\n",
    "# Save the TorchScript model (optional, for debugging)\n",
    "traced_model_path = \"/Users/yahyarahhawi/Developer/Film/cycleGAN_traced.pt\"\n",
    "traced_model.save(traced_model_path)\n",
    "\n",
    "print(f\"TorchScript model saved to {traced_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "# Convert the TorchScript model to Core ML\n",
    "coreml_model = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[ct.ImageType(name=\"input\", shape=(1, 3, 256, 256))],\n",
    "    minimum_deployment_target=ct.target.iOS13  # Specify deployment target if needed\n",
    ")\n",
    "\n",
    "# Save the Core ML model with the correct extension\n",
    "coreml_model_path = \"/Users/yahyarahhawi/Developer/Film/cycleGAN.mlpackage\"\n",
    "coreml_model.save(coreml_model_path)\n",
    "\n",
    "print(f\"Core ML model saved to {coreml_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test Core ML*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the Core ML model\n",
    "coreml_model_path = \"/Users/yahyarahhawi/Developer/Film/cycleGAN.mlpackage\"\n",
    "model = ct.models.MLModel(coreml_model_path)\n",
    "\n",
    "# Load an input image\n",
    "input_image_path = \"/Users/yahyarahhawi/Downloads/epoch017_real_A (1).png\"\n",
    "image = Image.open(input_image_path).convert(\"RGB\")\n",
    "image = image.resize((256, 256))  # Resize to the expected input size\n",
    "\n",
    "# Perform inference\n",
    "output = model.predict({\"input\": image})\n",
    "\n",
    "# Access the output using the correct key\n",
    "output_image_data = output[\"var_309\"]  # Use the correct key\n",
    "\n",
    "# Process the output: Convert from NumPy array to PIL Image\n",
    "output_image_data = (output_image_data.squeeze().transpose(1, 2, 0) * 255).clip(0, 255).astype(np.uint8)  # (H, W, C)\n",
    "output_image = Image.fromarray(output_image_data)\n",
    "\n",
    "# Save or display the output image\n",
    "output_image.save(\"/Users/yahyarahhawi/Developer/Film/output_image.jpg\")\n",
    "output_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
