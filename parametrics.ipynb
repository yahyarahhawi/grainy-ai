{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping:\n",
      "{'real_film': 0, 'real_iphone': 1}\n",
      "Epoch 1/50, Train Loss: 0.4789, Train Acc: 0.7570, Val Acc: 0.8497\n",
      "Epoch 2/50, Train Loss: 0.1568, Train Acc: 0.9525, Val Acc: 0.8889\n",
      "Epoch 3/50, Train Loss: 0.0706, Train Acc: 0.9832, Val Acc: 0.8954\n",
      "Epoch 4/50, Train Loss: 0.0364, Train Acc: 0.9944, Val Acc: 0.9150\n",
      "Epoch 5/50, Train Loss: 0.0179, Train Acc: 1.0000, Val Acc: 0.9346\n",
      "Epoch 6/50, Train Loss: 0.0134, Train Acc: 1.0000, Val Acc: 0.9542\n",
      "Epoch 7/50, Train Loss: 0.0071, Train Acc: 1.0000, Val Acc: 0.9412\n",
      "Epoch 8/50, Train Loss: 0.0059, Train Acc: 1.0000, Val Acc: 0.9608\n",
      "Epoch 9/50, Train Loss: 0.0149, Train Acc: 0.9958, Val Acc: 0.9477\n",
      "Epoch 10/50, Train Loss: 0.0105, Train Acc: 0.9986, Val Acc: 0.9477\n",
      "Epoch 11/50, Train Loss: 0.0090, Train Acc: 0.9986, Val Acc: 0.9150\n",
      "Epoch 12/50, Train Loss: 0.0118, Train Acc: 0.9972, Val Acc: 0.9020\n",
      "Epoch 13/50, Train Loss: 0.0252, Train Acc: 0.9930, Val Acc: 0.8824\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# ----- 1. Define your transforms -----\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ----- 2. Create a dataset from the evaluation directory -----\n",
    "root_dir = \"/Users/yahyarahhawi/Developer/Film/evaluation\"\n",
    "full_dataset = datasets.ImageFolder(root=root_dir, transform=train_transforms)\n",
    "\n",
    "# Optional: check class-to-index mapping\n",
    "print(\"Class to index mapping:\")\n",
    "print(full_dataset.class_to_idx)\n",
    "\n",
    "# ----- 3. Split into train, val, test (70%/15%/15%) -----\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Overwrite val/test transforms\n",
    "val_dataset.dataset.transform = val_test_transforms\n",
    "test_dataset.dataset.transform = val_test_transforms\n",
    "\n",
    "# ----- 4. Dataloaders -----\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ----- 5. Use pretrained ResNet18 with dropout -----\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        \n",
    "        # Custom classifier with Dropout to prevent overfitting\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(num_features, 2)  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "model = CustomResNet()\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "# ----- 6. Define loss and optimizer with weight decay -----\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "\n",
    "# ----- 7. Training loop with early stopping -----\n",
    "epochs = 50\n",
    "patience = 0\n",
    "best_val_acc = 0.0\n",
    "early_stop_threshold = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        patience = 0  # Reset patience if validation improves\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop_threshold:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/zqj4hsr94qs443gkb32p5h_00000gn/T/ipykernel_85300/2324738706.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-based FID (cyclegan film vs Real Film): 463.0740723128418\n",
      "ResNet-based FID (iPhone vs Real Film): 789.1210473913691\n",
      "ResNet-based FID (Diffusion film vs Real Film): 299.6404946109028\n",
      "ResNet-based FID (validation real film vs Real Film): 346.456256941291\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# ================================\n",
    "# 1. Device Setup\n",
    "# ================================\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2. Your Custom ResNet Model\n",
    "#    (the one you trained for film vs iPhone)\n",
    "# ================================\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 2)  # final layer: 2-class\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3. Feature Extractor\n",
    "#    (Removes final classification head -> 512-d features)\n",
    "# ================================\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, ckpt_path):\n",
    "        super().__init__()\n",
    "        # Load your trained ResNet\n",
    "        self.model = CustomResNet()\n",
    "        # Load the weights you saved; adjust path if needed\n",
    "        self.model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Replace final classification layers with Identity,\n",
    "        # so forward() returns the 512-d feature vector.\n",
    "        self.model.base_model.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4. Single-Folder Dataset\n",
    "#    (No subfolders required)\n",
    "# ================================\n",
    "class SingleImageFolder(Dataset):\n",
    "    \"\"\"\n",
    "    Reads all images from a single folder (no subfolders).\n",
    "    Returns (image, 0) because we don't need real labels for FID.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        valid_exts = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')\n",
    "        self.image_paths = [\n",
    "            os.path.join(root, fname)\n",
    "            for fname in os.listdir(root)\n",
    "            if fname.lower().endswith(valid_exts)\n",
    "        ]\n",
    "        self.image_paths.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. ResNet Transforms\n",
    "#    (224x224, standard ImageNet normalization)\n",
    "# ================================\n",
    "resnet_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 6. Compute Mean/Cov in ResNet Space\n",
    "# ================================\n",
    "def compute_statistics_of_folder(folder_path, model, transform, batch_size=32):\n",
    "    dataset = SingleImageFolder(root=folder_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    features = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_imgs, _ in dataloader:\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            preds = model(batch_imgs)  # shape: [batch_size, 512]\n",
    "            features.append(preds.cpu().numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    mu = np.mean(features, axis=0)\n",
    "    sigma = np.cov(features, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 7. \"FID-like\" Distance\n",
    "# ================================\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2):\n",
    "    diff = mu1 - mu2\n",
    "    diff_squared = diff.dot(diff)\n",
    "    \n",
    "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid_value = diff_squared + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid_value\n",
    "\n",
    "def compute_fid_resnet(folder1, folder2, model, transform, batch_size=32):\n",
    "    mu1, sigma1 = compute_statistics_of_folder(folder1, model, transform, batch_size)\n",
    "    mu2, sigma2 = compute_statistics_of_folder(folder2, model, transform, batch_size)\n",
    "    fid_val = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return fid_val\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 8. Instantiate the Model\n",
    "#    (Point to your trained weights)\n",
    "# ================================\n",
    "ckpt_path = \"/Users/yahyarahhawi/Developer/Film/best_model.pth\"  # Adjust if needed\n",
    "resnet_feature_extractor = ResNetFeatureExtractor(ckpt_path=ckpt_path).to(device)\n",
    "resnet_feature_extractor.eval()\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 9. Compare Various Folders\n",
    "# ================================\n",
    "# Modify these paths to match your folder structure\n",
    "path_cyclegan    = \"/Users/yahyarahhawi/Developer/Film/real_vs_fake/fake_cinestill_cyclegan\"\n",
    "path_real_film         = \"/Users/yahyarahhawi/Developer/Film/evaluation/real_film\"\n",
    "path_real_iphone       = \"/Users/yahyarahhawi/Developer/Film/real_vs_fake/real_iphone\"\n",
    "path_diffusion = \"/Users/yahyarahhawi/Developer/Film/real_vs_fake/fake_cinestill_diffusion\"\n",
    "path_validation_real_film = \"/Users/yahyarahhawi/Developer/Film/real_vs_fake/validation_real_film\"\n",
    "\n",
    "# 1) Fake Cinestill vs Real Film\n",
    "fid_fake_vs_film = compute_fid_resnet(\n",
    "    folder1=path_cyclegan,\n",
    "    folder2=path_real_film,\n",
    "    model=resnet_feature_extractor,\n",
    "    transform=resnet_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "print(\"ResNet-based FID (cyclegan film vs Real Film):\", fid_fake_vs_film)\n",
    "\n",
    "# 2) iPhone vs Real Film\n",
    "fid_iphone_vs_film = compute_fid_resnet(\n",
    "    folder1=path_real_iphone,\n",
    "    folder2=path_real_film,\n",
    "    model=resnet_feature_extractor,\n",
    "    transform=resnet_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "print(\"ResNet-based FID (iPhone vs Real Film):\", fid_iphone_vs_film)\n",
    "\n",
    "# 3) Diffusion vs Real Film\n",
    "fid_diffusion_vs_film = compute_fid_resnet(\n",
    "    folder1=path_diffusion,\n",
    "    folder2=path_real_film,\n",
    "    model=resnet_feature_extractor,\n",
    "    transform=resnet_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "print(\"ResNet-based FID (Diffusion film vs Real Film):\", fid_diffusion_vs_film)\n",
    "\n",
    "# 3) validation real film vs Real Film\n",
    "fid_validation_vs_film = compute_fid_resnet(\n",
    "    folder1=path_validation_real_film,\n",
    "    folder2=path_real_film,\n",
    "    model=resnet_feature_extractor,\n",
    "    transform=resnet_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "print(\"ResNet-based FID (validation real film vs Real Film):\", fid_validation_vs_film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
